{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c653b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response:  \"I appreciate you taking the time to share your concerns with me. I'm sorry to hear that you're not satisfied with your experience. Can you please provide more details about what went wrong so I can better understand the issue and see how I can improve it for you in the future?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Step 1: Load API key from .env file\n",
    "load_dotenv()\n",
    "api_key_01 = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Step 2: Select a parser for the output (in this case, a simple string parser)\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=api_key_01,\n",
    "    temperature=0.1 # Lower temperature for better JSON generation\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "class FeedbackAnalysis(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative', 'neutral'] = Field(..., description=\"The sentiment of the feedback\")\n",
    "    \n",
    "pyd_parser = PydanticOutputParser(pydantic_object=FeedbackAnalysis)\n",
    "\n",
    "prompt_01 = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following feedback text as positive, negative, or neutral.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "IMPORTANT: Return ONLY the raw JSON output. Do not include any python code, markdown formatting (like ```json), or explanations.\n",
    "\n",
    "Feedback: {feedback}\"\"\",\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instructions\": pyd_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "classifier_chain = prompt_01 | model | pyd_parser\n",
    "\n",
    "prompt_02 = PromptTemplate(\n",
    "    template='Write an exact appropiate response of the following positive feedback, dont give me feedback options\\n {feedback}',\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "prompt_03 = PromptTemplate(\n",
    "    template='Write an exact appropiate response of the following negitive feedback, dont give me feedback options\\n {feedback}',\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment == \"positive\", prompt_02 | model | str_parser),\n",
    "    (lambda x: x.sentiment == \"negative\", prompt_03 | model | str_parser),\n",
    "    RunnableLambda(lambda x: \"Neutral feedback, no response needed\")\n",
    ")\n",
    "\n",
    "\n",
    "chain = classifier_chain | branch_chain\n",
    "\n",
    "result = chain.invoke({\"feedback\": \"The product is just a holly shit, it broke after one use and the customer service was terrible!\"})\n",
    "\n",
    "print(\"Final Response: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f7c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "  +-----------------+    \n",
      "  | ChatHuggingFace |    \n",
      "  +-----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
