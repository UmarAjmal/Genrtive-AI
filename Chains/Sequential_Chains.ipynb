{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08acf1fb",
   "metadata": {},
   "source": [
    "# Sequential Chains in LangChain - Detailed Explanation\n",
    "\n",
    "## Sequential Chain Kya Hota Hai?\n",
    "\n",
    "**Sequential Chain** ek workflow hai jisme multiple chains ek-ek karke execute hote hain. Pehle chain ka output, doosre chain ka input ban jaata hai.\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential vs Simple Chain\n",
    "\n",
    "| Feature | Simple Chain | Sequential Chain |\n",
    "|---------|------------|-----------------|\n",
    "| **Steps** | 1 prompt ‚Üí 1 model | Multiple prompts ‚Üí Multiple models |\n",
    "| **Flow** | Linear (ek hi) | Step-by-step (multiple) |\n",
    "| **Complexity** | Simple | Advanced |\n",
    "| **Use Case** | Single task | Multi-step tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential Chain Ka Structure\n",
    "\n",
    "```\n",
    "Step 1: Prompt 1 ‚Üí Model 1 ‚Üí Parser 1 (Output A)\n",
    "         ‚Üì\n",
    "Step 2: Prompt 2 (Input A) ‚Üí Model 2 ‚Üí Parser 2 (Output B)\n",
    "         ‚Üì\n",
    "Step 3: Prompt 3 (Input B) ‚Üí Model 3 ‚Üí Parser 3 (Final Output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Example\n",
    "\n",
    "**Task:** Kisi book ke baare mein detailed summary likho\n",
    "\n",
    "**Sequential Steps:**\n",
    "\n",
    "1Ô∏è‚É£ **Step 1**: Book ke baare mein basic info generate karo  \n",
    "2Ô∏è‚É£ **Step 2**: Us info se detailed summary likho  \n",
    "3Ô∏è‚É£ **Step 3**: Summary se key takeaways nikalo  \n",
    "\n",
    "---\n",
    "\n",
    "## Code Example - Sequential Chain\n",
    "\n",
    "````python\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=200\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "# ============ STEP 1: Generate Book Info ============\n",
    "prompt_1 = PromptTemplate(\n",
    "    template=\"Provide basic information about the book '{book_name}' in 3 lines. Include author, genre, and publication year.\",\n",
    "    input_variables=[\"book_name\"]\n",
    ")\n",
    "\n",
    "chain_1 = prompt_1 | model | str_parser\n",
    "\n",
    "# ============ STEP 2: Generate Summary ============\n",
    "prompt_2 = PromptTemplate(\n",
    "    template=\"Based on this book information: {book_info}\\n\\nNow write a detailed summary of the book in 5 points.\",\n",
    "    input_variables=[\"book_info\"]\n",
    ")\n",
    "\n",
    "chain_2 = prompt_2 | model | str_parser\n",
    "\n",
    "# ============ STEP 3: Extract Key Takeaways ============\n",
    "prompt_3 = PromptTemplate(\n",
    "    template=\"From this summary: {summary}\\n\\nExtract 3 most important key takeaways from the book.\",\n",
    "    input_variables=[\"summary\"]\n",
    ")\n",
    "\n",
    "chain_3 = prompt_3 | model | str_parser\n",
    "\n",
    "# ============ EXECUTE SEQUENTIAL CHAIN ============\n",
    "print(\"=== STEP 1: Book Information ===\")\n",
    "book_info = chain_1.invoke({\"book_name\": \"1984\"})\n",
    "print(book_info)\n",
    "\n",
    "print(\"\\n=== STEP 2: Detailed Summary ===\")\n",
    "summary = chain_2.invoke({\"book_info\": book_info})\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n=== STEP 3: Key Takeaways ===\")\n",
    "takeaways = chain_3.invoke({\"summary\": summary})\n",
    "print(takeaways)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## Using RunnableSequence (Advanced)\n",
    "\n",
    "````python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Method 1: Manual Sequential Execution (Recommended for clarity)\n",
    "def execute_sequential(book_name):\n",
    "    info = chain_1.invoke({\"book_name\": book_name})\n",
    "    summary = chain_2.invoke({\"book_info\": info})\n",
    "    takeaways = chain_3.invoke({\"summary\": summary})\n",
    "    \n",
    "    return {\n",
    "        \"book_info\": info,\n",
    "        \"summary\": summary,\n",
    "        \"takeaways\": takeaways\n",
    "    }\n",
    "\n",
    "result = execute_sequential(\"To Kill a Mockingbird\")\n",
    "print(result)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## LangChain's SequentialChain (Older Method)\n",
    "\n",
    "````python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Define chains with memory\n",
    "chain_1 = LLMChain(llm=model, prompt=prompt_1, output_key=\"book_info\")\n",
    "chain_2 = LLMChain(llm=model, prompt=prompt_2, output_key=\"summary\")\n",
    "chain_3 = LLMChain(llm=model, prompt=prompt_3, output_key=\"takeaways\")\n",
    "\n",
    "# Create sequential chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2, chain_3],\n",
    "    input_variables=[\"book_name\"],\n",
    "    output_variables=[\"book_info\", \"summary\", \"takeaways\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = overall_chain({\"book_name\": \"1984\"})\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## Real Example: Article Writing Workflow\n",
    "\n",
    "````python\n",
    "# STEP 1: Generate Title\n",
    "prompt_title = PromptTemplate(\n",
    "    template=\"Generate an engaging article title about {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "chain_title = prompt_title | model | str_parser\n",
    "\n",
    "# STEP 2: Generate Outline\n",
    "prompt_outline = PromptTemplate(\n",
    "    template=\"Given the title '{title}', create a 5-point article outline.\",\n",
    "    input_variables=[\"title\"]\n",
    ")\n",
    "chain_outline = prompt_outline | model | str_parser\n",
    "\n",
    "# STEP 3: Write Full Article\n",
    "prompt_article = PromptTemplate(\n",
    "    template=\"Using this outline:\\n{outline}\\n\\nWrite a detailed article.\",\n",
    "    input_variables=[\"outline\"]\n",
    ")\n",
    "chain_article = prompt_article | model | str_parser\n",
    "\n",
    "# EXECUTE\n",
    "print(\"Step 1: Generating Title...\")\n",
    "title = chain_title.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "\n",
    "print(\"Step 2: Generating Outline...\")\n",
    "outline = chain_outline.invoke({\"title\": title})\n",
    "\n",
    "print(\"Step 3: Writing Article...\")\n",
    "article = chain_article.invoke({\"outline\": outline})\n",
    "\n",
    "print(\"\\n=== FINAL ARTICLE ===\")\n",
    "print(article)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential Chain Flow Diagram\n",
    "\n",
    "```\n",
    "INPUT: User Request\n",
    "    ‚Üì\n",
    "STEP 1: Process Input\n",
    "    ‚îú‚îÄ Prompt Template\n",
    "    ‚îú‚îÄ LLM Model\n",
    "    ‚îî‚îÄ Output Parser ‚Üí Output A\n",
    "    ‚Üì\n",
    "STEP 2: Use Output A as Input\n",
    "    ‚îú‚îÄ Prompt Template (with Output A)\n",
    "    ‚îú‚îÄ LLM Model\n",
    "    ‚îî‚îÄ Output Parser ‚Üí Output B\n",
    "    ‚Üì\n",
    "STEP 3: Use Output B as Input\n",
    "    ‚îú‚îÄ Prompt Template (with Output B)\n",
    "    ‚îú‚îÄ LLM Model\n",
    "    ‚îî‚îÄ Output Parser ‚Üí Final Output\n",
    "    ‚Üì\n",
    "OUTPUT: Final Result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Advantages\n",
    "\n",
    "‚úÖ **Complex Tasks** - Multi-step workflows ko handle karta hai  \n",
    "‚úÖ **Reusable** - Har step independently use ho sakta hai  \n",
    "‚úÖ **Debugging** - Har step ka output dekhna aasan  \n",
    "‚úÖ **Flexible** - Output of one step ‚Üí Input of next  \n",
    "‚úÖ **Professional** - Production-level applications  \n",
    "\n",
    "---\n",
    "\n",
    "## Common Use Cases\n",
    "\n",
    "1. **Content Generation**\n",
    "   - Title ‚Üí Outline ‚Üí Full Article\n",
    "\n",
    "2. **Question Answering**\n",
    "   - Extract Context ‚Üí Understand Question ‚Üí Generate Answer\n",
    "\n",
    "3. **Code Generation**\n",
    "   - Understand Request ‚Üí Plan Solution ‚Üí Generate Code\n",
    "\n",
    "4. **Research**\n",
    "   - Search Topic ‚Üí Summarize ‚Üí Create Report\n",
    "\n",
    "5. **Translation + Summarization**\n",
    "   - Translate Text ‚Üí Summarize ‚Üí Extract Key Points\n",
    "\n",
    "---\n",
    "\n",
    "## Error Handling in Sequential Chains\n",
    "\n",
    "````python\n",
    "def execute_with_error_handling(book_name):\n",
    "    try:\n",
    "        print(\"Step 1: Fetching book info...\")\n",
    "        info = chain_1.invoke({\"book_name\": book_name})\n",
    "        if not info:\n",
    "            raise ValueError(\"Step 1 failed: No book info generated\")\n",
    "        \n",
    "        print(\"Step 2: Generating summary...\")\n",
    "        summary = chain_2.invoke({\"book_info\": info})\n",
    "        if not summary:\n",
    "            raise ValueError(\"Step 2 failed: No summary generated\")\n",
    "        \n",
    "        print(\"Step 3: Extracting takeaways...\")\n",
    "        takeaways = chain_3.invoke({\"summary\": summary})\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"book_info\": info,\n",
    "            \"summary\": summary,\n",
    "            \"takeaways\": takeaways\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "result = execute_with_error_handling(\"1984\")\n",
    "print(result)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "üîπ **Sequential = Step-by-step execution**  \n",
    "üîπ **Output of Step N ‚Üí Input of Step N+1**  \n",
    "üîπ **Multiple prompts, multiple models**  \n",
    "üîπ **Excellent for complex workflows**  \n",
    "üîπ **Easy debugging and monitoring**  \n",
    "\n",
    "**Bas Itna Samajh Loo: Sequential Chain = Pehla chain ‚Üí Doosra chain ‚Üí Teesra chain** ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48334d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 5-pointer summary of the text:\n",
      "\n",
      "1. **Definition and Concept**: Black holes are regions of spacetime where the gravitational pull is so strong that nothing, not even light, can escape, and their concept has been around for centuries.\n",
      "\n",
      "2. **History of Black Holes**: The concept of black holes began to take shape in the 20th century with the work of scientists like Albert Einstein, David Finkelstein, Martin Schwarzschild, and Roger Penrose, who introduced key concepts like the event horizon.\n",
      "\n",
      "3. **Types of Black Holes**: There are four types of black holes: Stellar Black Holes (formed from massive star collapse), Intermediate-Mass Black Holes (formed from stellar black hole mergers), Supermassive Black Holes (found at galaxy centers), and Primordial Black Holes (hypothetical, formed in the early universe).\n",
      "\n",
      "4. **Properties of Black Holes**: Black holes have unique properties, such as their massive size, strong gravitational pull, and the event horizon, which marks the boundary beyond which nothing can escape.\n",
      "\n",
      "5. **Importance of Black Holes**: Understanding black holes is crucial in astronomy, as they are among the most fascinating and mysterious objects in the universe, offering insights into the behavior of matter and energy under extreme conditions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Step 1: Load API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Step 2: Select a parser for the output (in this case, a simple string parser)\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "# Step 3: Initialize the HuggingFaceEndpoint with the desired model and task\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=api_key\n",
    ")\n",
    "\n",
    "# Step 4: Create the first prompt to generate a detailed report on a given topic\n",
    "prompt_01 = PromptTemplate(\n",
    "    template = \"Generate a detailed report on this topic: {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Step 5: Create the second prompt to summarize the generated report in JSON format\n",
    "prompt_02 = PromptTemplate(\n",
    "    template= \"Generate a 5 pointer summary of the following text{text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Step 6: Chain the prompts and the model together\n",
    "model = ChatHuggingFace(llm = llm)\n",
    "\n",
    "# Step 7: Form the complete chain\n",
    "chain =(\n",
    "    prompt_01\n",
    "    | model\n",
    "    | str_parser\n",
    "    | prompt_02\n",
    "    | model\n",
    "    | str_parser\n",
    ")\n",
    "\n",
    "#Step 8: Invoke the chain with a specific topic and print the result\n",
    "result = chain.invoke({\"topic\": \"Black Holes\"})\n",
    "\n",
    "# Step 9: Print the final result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c18892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
